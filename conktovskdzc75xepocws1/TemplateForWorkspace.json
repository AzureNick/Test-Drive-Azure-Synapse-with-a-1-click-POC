{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "conktovskdzc75xepocws1"
		},
		"TripFaresSynapseAnalyticsLinkedService_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'TripFaresSynapseAnalyticsLinkedService'"
		},
		"RawTipDataSource_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'RawTipDataSource'"
		},
		"HttpServerTripDataLinkedService_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://raw.githubusercontent.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/main/tripDataAndFaresCSV/trip-data.csv"
		},
		"HttpServerTripFareDataLinkedService_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://raw.githubusercontent.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/main/tripDataAndFaresCSV/fares-data.csv"
		},
		"TripFaresDataLakeStorageLinkedService_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "@{concat('https://',linkedService().datalakeAccountName,'.dfs.core.windows.net')}"
		},
		"keyVaultLinkedservice_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "@{concat('https://',linkedService().keyVaultName,'.vault.azure.net/')}"
		},
		"TipSourceData_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://synapsetiphackathonadls.dfs.core.windows.net"
		},
		"RawTipDataSource_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://synapsetiphackathonadls.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Load Dataframe a bunch')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Just to generate a little load for monitoring",
				"activities": [
					{
						"name": "EnrichDataPipeline",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "EnrichData",
								"type": "NotebookReference"
							},
							"parameters": {
								"tableName": {
									"value": "customer",
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "ws1sparkpool1",
								"type": "BigDataPoolReference"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/EnrichData')]",
				"[concat(variables('workspaceId'), '/bigDataPools/ws1sparkpool1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PauseSqlPool')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "GET List",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Synapse/workspaces/',pipeline().parameters.WorkspaceName,'/sqlPools?api-version=2019-06-01-preview')",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "GET",
							"headers": {},
							"authentication": {
								"type": "MSI",
								"resource": "https://management.azure.com/"
							}
						}
					},
					{
						"name": "FilterProd",
						"type": "Filter",
						"dependsOn": [
							{
								"activity": "GET List",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get list').output.value",
								"type": "Expression"
							},
							"condition": {
								"value": "@not(endswith(item().name,'conktovskdzc75xepocws1'))",
								"type": "Expression"
							}
						}
					},
					{
						"name": "ForEach_Pool",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "FilterProd",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('FilterProd').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "CheckState",
									"type": "WebActivity",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Synapse/workspaces/',pipeline().parameters.WorkspaceName,'/sqlPools/',item().name,'?api-version=2019-06-01-preview')",
											"type": "Expression"
										},
										"connectVia": {
											"referenceName": "AutoResolveIntegrationRuntime",
											"type": "IntegrationRuntimeReference"
										},
										"method": "GET",
										"headers": {},
										"authentication": {
											"type": "MSI",
											"resource": "https://management.azure.com/"
										}
									}
								},
								{
									"name": "StatePauseOrResume",
									"type": "Switch",
									"dependsOn": [
										{
											"activity": "CheckState",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"on": {
											"value": "@concat(activity('CheckState').output.properties.status,'-',pipeline().parameters.PauseOrResume)",
											"type": "Expression"
										},
										"cases": [
											{
												"value": "Online-Pause",
												"activities": [
													{
														"name": "SQLPoolPause",
														"type": "WebActivity",
														"dependsOn": [],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"url": {
																"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Synapse/workspaces/',pipeline().parameters.WorkspaceName,'/sqlPools/',activity('CheckState').output.name,'/pause?api-version=2019-06-01-preview')",
																"type": "Expression"
															},
															"connectVia": {
																"referenceName": "AutoResolveIntegrationRuntime",
																"type": "IntegrationRuntimeReference"
															},
															"method": "POST",
															"headers": {},
															"body": "Pause and Resume",
															"authentication": {
																"type": "MSI",
																"resource": "https://management.azure.com/"
															}
														}
													}
												]
											}
										]
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"ResourceGroup": {
						"type": "string",
						"defaultValue": "ContosoProd"
					},
					"SubscriptionID": {
						"type": "string",
						"defaultValue": "9cd45dfd-d1ac-4c16-9363-150d2afcb462"
					},
					"WorkspaceName": {
						"type": "string",
						"defaultValue": "9cd45dfd-d1ac-4c16-9363-150d2afcb462"
					},
					"SQLPoolName": {
						"type": "string",
						"defaultValue": "conktovskdzc75xepocws1p1"
					},
					"PauseorResume": {
						"type": "string",
						"defaultValue": "Pause"
					}
				},
				"folder": {
					"name": "ResourceManagement"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RunAggregationWithStats')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "AggregateTripFaresData_withStats",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "conktovskdzc75xepocws1p1",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[AggregateDuplicatedTripFareData_withStats]"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "Generate SQL Pool Workload"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/conktovskdzc75xepocws1p1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RunAggregationWithoutStats')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "AggregateTripFaresData_noStats",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "conktovskdzc75xepocws1p1",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[AggregateDuplicatedTripFareData_noStats]"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "Generate SQL Pool Workload"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/conktovskdzc75xepocws1p1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TripFaresDataPipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "IngestTripDataIntoADLS",
						"description": "Copies the trip data csv file from the git repo and loads it into the ADLS.",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.00:10:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "tripsDataSource",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "tripDataSink",
								"type": "DatasetReference",
								"parameters": {
									"datalakeAccountName": {
										"value": "@pipeline().parameters.datalakeAccountName",
										"type": "Expression"
									},
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "IngestTripFaresDataIntoADLS",
						"description": "Copies the trip fare data csv file from the git repo and loads it into the ADLS.",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.00:10:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "faresDataSource",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "faresDataSink",
								"type": "DatasetReference",
								"parameters": {
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									},
									"datalakeAccountName": {
										"value": "@pipeline().parameters.datalakeAccountName",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "JoinAndAggregateData",
						"description": "Reads the raw data from both CSV files inside the ADLS, performs the desired transformations (inner join and aggregation) and writes the transformed data into the synapse SQL pool.",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Create Schema If Does Not Exists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.00:30:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "tripFaresDataTransformations",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"TripDataCSV": {
										"datalakeAccountName": {
											"value": "@pipeline().parameters.datalakeAccountName",
											"type": "Expression"
										},
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										}
									},
									"FaresDataCSV": {
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										},
										"datalakeAccountName": {
											"value": "@pipeline().parameters.datalakeAccountName",
											"type": "Expression"
										}
									},
									"SynapseAnalyticsSink": {
										"SchemaName": {
											"value": "@pipeline().parameters.SchemaName",
											"type": "Expression"
										},
										"SynapseWorkspaceName": {
											"value": "@pipeline().parameters.SynapseWorkspaceName",
											"type": "Expression"
										},
										"SQLDedicatedPoolName": {
											"value": "@pipeline().parameters.SQLDedicatedPoolName",
											"type": "Expression"
										},
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										},
										"SQLLoginUsername": {
											"value": "@pipeline().parameters.SQLLoginUsername",
											"type": "Expression"
										}
									}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Create Schema If Does Not Exists",
						"description": "Creates the schema inside the SQL dedicated pool. Shema name comes from the pipeline parameter 'SchemaName'.",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "IngestTripDataIntoADLS",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "IngestTripFaresDataIntoADLS",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.00:05:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlDWSource",
								"sqlReaderQuery": {
									"value": "IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = '@{pipeline().parameters.SchemaName}')\nBEGIN\nEXEC('CREATE SCHEMA @{pipeline().parameters.SchemaName}')\nselect Count(*) from sys.symmetric_keys;\nEND\nELSE\nBEGIN\n    select Count(*) from sys.symmetric_keys;\nEND",
									"type": "Expression"
								},
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "azureSynapseAnalyticsSchema",
								"type": "DatasetReference",
								"parameters": {
									"SynapseWorkspaceName": {
										"value": "@pipeline().parameters.SynapseWorkspaceName",
										"type": "Expression"
									},
									"SQLDedicatedPoolName": {
										"value": "@pipeline().parameters.SQLDedicatedPoolName",
										"type": "Expression"
									},
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									},
									"SQLLoginUsername": {
										"value": "@pipeline().parameters.SQLLoginUsername",
										"type": "Expression"
									}
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Copy data Trips Data",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Create Schema If Does Not Exists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "SqlDWSink",
								"preCopyScript": "IF (EXISTS (SELECT *\n  FROM INFORMATION_SCHEMA.TABLES\n  WHERE TABLE_SCHEMA = 'dbo'\n  AND TABLE_NAME = 'TripsData'))\nBEGIN \n   Truncate table TripsData;\nEnd\n",
								"allowPolyBase": true,
								"polyBaseSettings": {
									"rejectValue": 0,
									"rejectType": "value",
									"useTypeDefault": true
								},
								"tableOption": "autoCreate",
								"disableMetricsCollection": false
							},
							"enableStaging": true,
							"stagingSettings": {
								"linkedServiceName": {
									"referenceName": "TripFaresDataLakeStorageLinkedService",
									"type": "LinkedServiceReference",
									"parameters": {
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										},
										"datalakeAccountName": {
											"value": "@pipeline().parameters.datalakeAccountName",
											"type": "Expression"
										}
									}
								}
							}
						},
						"inputs": [
							{
								"referenceName": "tripsDataSource",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "AzureSynapseAnalyticsTripsData",
								"type": "DatasetReference",
								"parameters": {
									"SynapseWorkspaceName": {
										"value": "@pipeline().parameters.SynapseWorkspaceName",
										"type": "Expression"
									},
									"SQLDedicatedPoolName": {
										"value": "@pipeline().parameters.SQLDedicatedPoolName",
										"type": "Expression"
									},
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									},
									"SQLLoginUsername": {
										"value": "@pipeline().parameters.SQLLoginUsername",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "Copy data Fares Data",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Create Schema If Does Not Exists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "SqlDWSink",
								"preCopyScript": "IF (EXISTS (SELECT *\n  FROM INFORMATION_SCHEMA.TABLES\n  WHERE TABLE_SCHEMA = 'dbo'\n  AND TABLE_NAME = 'FaresData'))\nBEGIN \n   Truncate table FaresData;\nEnd\n",
								"allowPolyBase": true,
								"polyBaseSettings": {
									"rejectValue": 0,
									"rejectType": "value",
									"useTypeDefault": true
								},
								"tableOption": "autoCreate",
								"disableMetricsCollection": false
							},
							"enableStaging": true,
							"stagingSettings": {
								"linkedServiceName": {
									"referenceName": "TripFaresDataLakeStorageLinkedService",
									"type": "LinkedServiceReference",
									"parameters": {
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										},
										"datalakeAccountName": {
											"value": "@pipeline().parameters.datalakeAccountName",
											"type": "Expression"
										}
									}
								}
							}
						},
						"inputs": [
							{
								"referenceName": "faresDataSource",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "AzureSynapseAnalyticsFaresData",
								"type": "DatasetReference",
								"parameters": {
									"SynapseWorkspaceName": {
										"value": "@pipeline().parameters.SynapseWorkspaceName",
										"type": "Expression"
									},
									"SQLDedicatedPoolName": {
										"value": "@pipeline().parameters.SQLDedicatedPoolName",
										"type": "Expression"
									},
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									},
									"SQLLoginUsername": {
										"value": "@pipeline().parameters.SQLLoginUsername",
										"type": "Expression"
									}
								}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"SchemaName": {
						"type": "string",
						"defaultValue": "tripFares"
					},
					"SynapseWorkspaceName": {
						"type": "string",
						"defaultValue": "conktovskdzc75xepocws1.database.windows.net"
					},
					"SQLDedicatedPoolName": {
						"type": "string",
						"defaultValue": "conktovskdzc75xepocws1p1"
					},
					"SQLLoginUsername": {
						"type": "string",
						"defaultValue": "nicksalc"
					},
					"KeyVaultName": {
						"type": "string",
						"defaultValue": "kvconktovskdzc75xepoc"
					},
					"datalakeAccountName": {
						"type": "string",
						"defaultValue": "conktovskdzc75xepoc"
					}
				},
				"folder": {
					"name": "TripFaresDataPipeline"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/tripsDataSource')]",
				"[concat(variables('workspaceId'), '/datasets/tripDataSink')]",
				"[concat(variables('workspaceId'), '/datasets/faresDataSource')]",
				"[concat(variables('workspaceId'), '/datasets/faresDataSink')]",
				"[concat(variables('workspaceId'), '/dataflows/tripFaresDataTransformations')]",
				"[concat(variables('workspaceId'), '/datasets/azureSynapseAnalyticsSchema')]",
				"[concat(variables('workspaceId'), '/datasets/AzureSynapseAnalyticsTripsData')]",
				"[concat(variables('workspaceId'), '/datasets/AzureSynapseAnalyticsFaresData')]",
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalyticsFaresData')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"SynapseWorkspaceName": {
							"value": "@dataset().SynapseWorkspaceName",
							"type": "Expression"
						},
						"SQLDedicatedPoolName": {
							"value": "@dataset().SQLDedicatedPoolName",
							"type": "Expression"
						},
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"SQLLoginUsername": {
							"value": "@dataset().SQLLoginUsername",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"table": "FaresData"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalyticsTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalyticsTripsData')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"SynapseWorkspaceName": {
							"value": "@dataset().SynapseWorkspaceName",
							"type": "Expression"
						},
						"SQLDedicatedPoolName": {
							"value": "@dataset().SQLDedicatedPoolName",
							"type": "Expression"
						},
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"SQLLoginUsername": {
							"value": "@dataset().SQLLoginUsername",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"table": "TripsData"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/azureSynapseAnalyticsSchema')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"SynapseWorkspaceName": {
							"value": "@dataset().SynapseWorkspaceName",
							"type": "Expression"
						},
						"SQLDedicatedPoolName": {
							"value": "@dataset().SQLDedicatedPoolName",
							"type": "Expression"
						},
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"SQLLoginUsername": {
							"value": "@dataset().SQLLoginUsername",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/azureSynapseAnalyticsTable')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"SynapseWorkspaceName": {
							"value": "@dataset().SynapseWorkspaceName",
							"type": "Expression"
						},
						"SQLDedicatedPoolName": {
							"value": "@dataset().SQLDedicatedPoolName",
							"type": "Expression"
						},
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"SQLLoginUsername": {
							"value": "@dataset().SQLLoginUsername",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"SchemaName": {
						"type": "string"
					},
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().SchemaName",
						"type": "Expression"
					},
					"table": "AggregateTaxiData"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/faresDataSink')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresDataLakeStorageLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"datalakeAccountName": {
							"value": "@dataset().datalakeAccountName",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"keyVaultName": {
						"type": "string",
						"defaultValue": "kvmsft"
					},
					"datalakeAccountName": {
						"type": "string",
						"defaultValue": "adlsmsft"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "fares-data.csv",
						"fileSystem": "public"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "medallion",
						"type": "String"
					},
					{
						"name": "hack_license",
						"type": "String"
					},
					{
						"name": "vendor_id",
						"type": "String"
					},
					{
						"name": "pickup_datetime",
						"type": "String"
					},
					{
						"name": "payment_type",
						"type": "String"
					},
					{
						"name": "fare_amount",
						"type": "String"
					},
					{
						"name": "surcharge",
						"type": "String"
					},
					{
						"name": "mta_tax",
						"type": "String"
					},
					{
						"name": "tip_amount",
						"type": "String"
					},
					{
						"name": "tolls_amount",
						"type": "String"
					},
					{
						"name": "total_amount",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/faresDataSource')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "HttpServerTripFareDataLinkedService",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/HttpServerTripFareDataLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tripDataSink')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresDataLakeStorageLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"datalakeAccountName": {
							"value": "@dataset().datalakeAccountName",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"datalakeAccountName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "trip-data.csv",
						"fileSystem": "public"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "medallion",
						"type": "String"
					},
					{
						"name": "hack_license",
						"type": "String"
					},
					{
						"name": "vendor_id",
						"type": "String"
					},
					{
						"name": "rate_code",
						"type": "String"
					},
					{
						"name": "store_and_fwd_flag",
						"type": "String"
					},
					{
						"name": "pickup_datetime",
						"type": "String"
					},
					{
						"name": "dropoff_datetime",
						"type": "String"
					},
					{
						"name": "passenger_count",
						"type": "String"
					},
					{
						"name": "trip_time_in_secs",
						"type": "String"
					},
					{
						"name": "trip_distance",
						"type": "String"
					},
					{
						"name": "pickup_longitude",
						"type": "String"
					},
					{
						"name": "pickup_latitude",
						"type": "String"
					},
					{
						"name": "dropoff_longitude",
						"type": "String"
					},
					{
						"name": "dropoff_latitude",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tripsDataSource')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "HttpServerTripDataLinkedService",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/HttpServerTripDataLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HttpServerTripDataLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('HttpServerTripDataLinkedService_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HttpServerTripFareDataLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('HttpServerTripFareDataLinkedService_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PowerBIWorkspaceTripsFares')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "PowerBIWorkspace",
				"typeProperties": {
					"workspaceID": "",
					"tenantID": "72f988bf-86f1-41af-91ab-2d7cd011db47"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TripFaresDataLakeStorageLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"keyVaultName": {
						"type": "string"
					},
					"datalakeAccountName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('TripFaresDataLakeStorageLinkedService_properties_typeProperties_url')]",
					"accountKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "keyVaultLinkedservice",
							"type": "LinkedServiceReference",
							"parameters": {
								"keyVaultName": {
									"value": "@linkedService().keyVaultName",
									"type": "Expression"
								}
							}
						},
						"secretName": "adlsAccessKey"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/keyVaultLinkedservice')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TripFaresSynapseAnalyticsLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('TripFaresSynapseAnalyticsLinkedService_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "keyVaultLinkedservice",
							"type": "LinkedServiceReference",
							"parameters": {
								"keyVaultName": {
									"value": "@linkedService().keyVaultName",
									"type": "Expression"
								}
							}
						},
						"secretName": "synapseSqlLoginPassword"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/keyVaultLinkedservice')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/keyVaultLinkedservice')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"keyVaultName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('keyVaultLinkedservice_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0,
							"cleanup": true
						}
					}
				},
				"managedVirtualNetwork": {
					"type": "ManagedVirtualNetworkReference",
					"referenceName": "default"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tripFaresDataTransformations')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TripFaresDataFlow"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "tripDataSink",
								"type": "DatasetReference"
							},
							"name": "TripDataCSV"
						},
						{
							"dataset": {
								"referenceName": "faresDataSink",
								"type": "DatasetReference"
							},
							"name": "FaresDataCSV"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "azureSynapseAnalyticsTable",
								"type": "DatasetReference"
							},
							"name": "SynapseAnalyticsSink"
						}
					],
					"transformations": [
						{
							"name": "AggregateByPaymentType"
						},
						{
							"name": "InnerJoinWithTripFares"
						}
					],
					"script": "source(output(\n\t\tmedallion as string,\n\t\thack_license as string,\n\t\tvendor_id as string,\n\t\trate_code as string,\n\t\tstore_and_fwd_flag as string,\n\t\tpickup_datetime as string,\n\t\tdropoff_datetime as string,\n\t\tpassenger_count as string,\n\t\ttrip_time_in_secs as string,\n\t\ttrip_distance as string,\n\t\tpickup_longitude as string,\n\t\tpickup_latitude as string,\n\t\tdropoff_longitude as string,\n\t\tdropoff_latitude as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tinferDriftedColumnTypes: true,\n\tignoreNoFilesFound: false) ~> TripDataCSV\nsource(output(\n\t\tmedallion as string,\n\t\thack_license as string,\n\t\tvendor_id as string,\n\t\tpickup_datetime as string,\n\t\tpayment_type as string,\n\t\tfare_amount as string,\n\t\tsurcharge as string,\n\t\tmta_tax as string,\n\t\ttip_amount as string,\n\t\ttolls_amount as string,\n\t\ttotal_amount as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tinferDriftedColumnTypes: true,\n\tignoreNoFilesFound: false) ~> FaresDataCSV\nInnerJoinWithTripFares aggregate(groupBy(payment_type),\n\taverage_fare = avg(toInteger(total_amount)),\n\t\ttotal_trip_distance = sum(toInteger(trip_distance))) ~> AggregateByPaymentType\nTripDataCSV, FaresDataCSV join(TripDataCSV@medallion == FaresDataCSV@medallion\n\t&& TripDataCSV@hack_license == FaresDataCSV@hack_license\n\t&& TripDataCSV@vendor_id == FaresDataCSV@vendor_id\n\t&& TripDataCSV@pickup_datetime == FaresDataCSV@pickup_datetime,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~> InnerJoinWithTripFares\nAggregateByPaymentType sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\trecreate:true,\n\tformat: 'table',\n\tstaged: false,\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true,\n\terrorHandlingOption: 'stopOnFirstError') ~> SynapseAnalyticsSink"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/tripDataSink')]",
				"[concat(variables('workspaceId'), '/datasets/faresDataSink')]",
				"[concat(variables('workspaceId'), '/datasets/azureSynapseAnalyticsTable')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://conktovskdzc75xepoc.dfs.core.windows.net/nyctaxidata/NYCTripSmall.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 3')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE PROCEDURE AggregateDuplicatedTripFareData_noStats\nAS\n    --increase data size then run an aggregation. Trying to trigger some data movement while we are at it\n    --at DW100 this took about 12 minutes to run\n\n    --Make the data bigger in TempDB\n    CREATE TABLE #DuplicatedFares\n    WITH (Distribution = ROUND_ROBIN, HEAP)\n    AS\n    (\n        SELECT * FROM FaresData\n    )\n    INSERT INTO #DuplicatedFares SELECT * FROM #DuplicatedFares\n    INSERT INTO #DuplicatedFares SELECT * FROM #DuplicatedFares\n    INSERT INTO #DuplicatedFares SELECT * FROM #DuplicatedFares\n    INSERT INTO #DuplicatedFares SELECT * FROM #DuplicatedFares\n    INSERT INTO #DuplicatedFares SELECT * FROM #DuplicatedFares\n    INSERT INTO #DuplicatedFares SELECT * FROM #DuplicatedFares\n    SELECT COUNT(*) FROM #DuplicatedFares\n\n    CREATE TABLE #DuplicatedTrips\n    WITH (Distribution = ROUND_ROBIN, HEAP)\n    AS\n    (\n        SELECT * FROM TripsData\n    )\n    INSERT INTO #DuplicatedTrips SELECT * FROM #DuplicatedTrips\n    INSERT INTO #DuplicatedTrips SELECT * FROM #DuplicatedTrips\n    INSERT INTO #DuplicatedTrips SELECT * FROM #DuplicatedTrips\n    INSERT INTO #DuplicatedTrips SELECT * FROM #DuplicatedTrips\n    INSERT INTO #DuplicatedTrips SELECT * FROM #DuplicatedTrips\n    INSERT INTO #DuplicatedTrips SELECT * FROM #DuplicatedTrips\n    SELECT COUNT(*) FROM #DuplicatedTrips\n\n    --Aggregate fares by passenger count\n    SELECT count(T.Medallion) as Number_Trips,\n    sum(convert(DECIMAL(16,4), F.fare_amount)) AS FareAmount,\n    convert(INT, T.passenger_count) AS PassengerCount,\n    avg(convert(DECIMAL(16,4), F.fare_amount)) AS AvgFare\n    FROM #DuplicatedTrips T\n    JOIN #DuplicatedFares F \n    on T.medallion = F.medallion\n    AND t.pickup_datetime = f.pickup_datetime\n    GROUP BY T.passenger_count\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "conktovskdzc75xepocws1p1",
						"poolName": "conktovskdzc75xepocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 4')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE PROCEDURE AggregateDuplicatedTripFareData_withStats\nAS\n    --increase data size then run an aggregation. Trying to trigger some data movement while we are at it\n    --at DW100 this took about __ minutes to run\n\n    --Make the data bigger in TempDB\n    CREATE TABLE #DuplicatedFares\n    WITH (Distribution = ROUND_ROBIN, HEAP)\n    AS\n    (\n        SELECT * FROM FaresData\n    )\n    INSERT INTO #DuplicatedFares SELECT * FROM #DuplicatedFares\n    INSERT INTO #DuplicatedFares SELECT * FROM #DuplicatedFares\n    INSERT INTO #DuplicatedFares SELECT * FROM #DuplicatedFares\n    INSERT INTO #DuplicatedFares SELECT * FROM #DuplicatedFares\n    INSERT INTO #DuplicatedFares SELECT * FROM #DuplicatedFares\n    INSERT INTO #DuplicatedFares SELECT * FROM #DuplicatedFares\n    SELECT COUNT(*) FROM #DuplicatedFares\n\nCREATE STATISTICS fare_amount ON #DuplicatedFares (fare_amount); \nCREATE STATISTICS hack_license ON #DuplicatedFares (hack_license); \nCREATE STATISTICS medallion ON #DuplicatedFares (medallion); \nCREATE STATISTICS mta_tax ON #DuplicatedFares (mta_tax); \nCREATE STATISTICS payment_type ON #DuplicatedFares (payment_type); \nCREATE STATISTICS pickup_datetime ON #DuplicatedFares (pickup_datetime); \nCREATE STATISTICS surcharge ON #DuplicatedFares (surcharge); \nCREATE STATISTICS tip_amount ON #DuplicatedFares (tip_amount); \nCREATE STATISTICS tolls_amount ON #DuplicatedFares (tolls_amount); \nCREATE STATISTICS total_amount ON #DuplicatedFares (total_amount); \nCREATE STATISTICS vendor_id ON #DuplicatedFares (vendor_id);\n\n\n    CREATE TABLE #DuplicatedTrips\n    WITH (Distribution = ROUND_ROBIN, HEAP)\n    AS\n    (\n        SELECT * FROM TripsData\n    )\n    INSERT INTO #DuplicatedTrips SELECT * FROM #DuplicatedTrips\n    INSERT INTO #DuplicatedTrips SELECT * FROM #DuplicatedTrips\n    INSERT INTO #DuplicatedTrips SELECT * FROM #DuplicatedTrips\n    INSERT INTO #DuplicatedTrips SELECT * FROM #DuplicatedTrips\n    INSERT INTO #DuplicatedTrips SELECT * FROM #DuplicatedTrips\n    INSERT INTO #DuplicatedTrips SELECT * FROM #DuplicatedTrips\n    SELECT COUNT(*) FROM #DuplicatedTrips\n\n    CREATE STATISTICS dropoff_datetime ON #DuplicatedTrips (dropoff_datetime); \n    CREATE STATISTICS medallion ON #DuplicatedTrips (medallion); \n    CREATE STATISTICS hack_license ON #DuplicatedTrips (hack_license); \n\n    --Aggregate fares by passenger count\n    SELECT count(T.Medallion) as Number_Trips,\n    sum(convert(DECIMAL(16,4), F.fare_amount)) AS FareAmount,\n    convert(INT, T.passenger_count) AS PassengerCount,\n    avg(convert(DECIMAL(16,4), F.fare_amount)) AS AvgFare\n    FROM #DuplicatedTrips T\n    JOIN #DuplicatedFares F \n    on T.medallion = F.medallion\n    AND t.pickup_datetime = f.pickup_datetime\n    GROUP BY T.passenger_count\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "conktovskdzc75xepocws1p1",
						"poolName": "conktovskdzc75xepocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 5')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT * FROM sys.dm_pdw_exec_requests\nwhere status not in ('completed','cancelled','failed')\nORDER BY total_elapsed_time",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "conktovskdzc75xepocws1p1",
						"poolName": "conktovskdzc75xepocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CatalystOptimizer')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TIP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "ws1sparkpool1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4ad0c33c-b082-4b33-9648-3a1a7b0cf59d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/9cd45dfd-d1ac-4c16-9363-150d2afcb462/resourceGroups/ContosoProd/providers/Microsoft.Synapse/workspaces/conktovskdzc75xepocws1/bigDataPools/ws1sparkpool1",
						"name": "ws1sparkpool1",
						"type": "Spark",
						"endpoint": "https://conktovskdzc75xepocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ws1sparkpool1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\r\n",
							"// val dfProduct = spark.read.synapsesql(\"dedicatedsqlpool.cso.DimProduct\")\r\n",
							"// dfProduct.write.mode(\"overwrite\").format(\"delta\").save(\"/data/contoso/DimProduct/\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\r\n",
							"// import org.apache.spark.sql.functions._\r\n",
							"// val dfFactOnlineSales = spark.read.synapsesql(\"dedicatedsqlpool.cso.FactOnlineSales\")\r\n",
							"// val dfFactOnlineSales1 = dfFactOnlineSales.withColumn(\"SaleYear\", year(dfFactOnlineSales(\"DateKey\")))\r\n",
							"// dfFactOnlineSales1.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"SaleYear\").save(\"/data/contoso/FactOnlineSales/\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dfProduct = spark.read.format(\"delta\").load(\"/data/contoso/DimProduct/\")\r\n",
							"dfProduct.createOrReplaceTempView(\"DimProduct\")\r\n",
							"\r\n",
							"dfFactOnlineSales = spark.read.format(\"delta\").load(\"/data/contoso/FactOnlineSales/\")\r\n",
							"dfFactOnlineSales.createOrReplaceTempView(\"FactOnlineSales\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"dfJoinedSale = dfProduct.join(dfFactOnlineSales, dfProduct.ProductKey == dfFactOnlineSales.ProductKey) \r\n",
							"dfJoinedSale = dfJoinedSale.filter(dfJoinedSale.SaleYear == 2009)\r\n",
							"# display(dfJoinedSale)\r\n",
							"\r\n",
							"dfJoinedSale.explain() # only physical plan\r\n",
							"# dfJoinedSale.explain(True) # all plans including logical and physical plan"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"dfAggregatedSale = dfJoinedSale.groupBy('BrandName').agg({'SalesAmount':'sum'})\r\n",
							"\r\n",
							"dfAggregatedSale.explain()\r\n",
							"\r\n",
							"# display(dfAggregatedSale)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"EXPLAIN\r\n",
							"SELECT  SUM(f.SalesAmount) AS sales_by_brand_amount\r\n",
							",       p.BrandName\r\n",
							"FROM    FactOnlineSales AS f\r\n",
							"JOIN    DimProduct      AS p ON f.ProductKey = p.ProductKey\r\n",
							"GROUP BY p.BrandName"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")) # 26214400 - 26 MB\r\n",
							"\r\n",
							"# df1.join(broadcast(df2), Seq(\"id\")).explain #broadcast join hint"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Adaptive Query Execution https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution\r\n",
							"print(spark.conf.get(\"spark.sql.adaptive.enabled\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT  SUM(f.SalesAmount) AS sales_by_brand_amount\r\n",
							",       p.BrandName\r\n",
							"FROM    FactOnlineSales AS f\r\n",
							"JOIN    DimProduct      AS p ON f.ProductKey = p.ProductKey\r\n",
							"GROUP BY p.BrandName"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Data Exploration and ML Modeling - NYC taxi predict using Spark MLlib')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "ws1sparkpool1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fcda2899-0617-4610-9f1f-c44816d794e3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/9cd45dfd-d1ac-4c16-9363-150d2afcb462/resourceGroups/ContosoProd/providers/Microsoft.Synapse/workspaces/conktovskdzc75xepocws1/bigDataPools/ws1sparkpool1",
						"name": "ws1sparkpool1",
						"type": "Spark",
						"endpoint": "https://conktovskdzc75xepocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ws1sparkpool1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Predict NYC Taxi Tips using Spark ML and Azure Open Datasets\n",
							"\n",
							"The notebook ingests, visualizes, prepares and then trains a model based on an Open Dataset that tracks NYC Yellow Taxi trips and various attributes around them.\n",
							"The goal is to predict for a given trip whether there will be a tip or not.\n",
							"\n",
							" https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-machine-learning-mllib-notebook\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import matplotlib.pyplot as plt\n",
							"\n",
							"from pyspark.sql.functions import unix_timestamp\n",
							"\n",
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"from pyspark.ml import Pipeline\n",
							"from pyspark.ml import PipelineModel\n",
							"from pyspark.ml.feature import RFormula\n",
							"from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
							"from pyspark.ml.classification import LogisticRegression\n",
							"from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
							"from pyspark.ml.evaluation import BinaryClassificationEvaluator"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Ingest Data \n",
							"\n",
							"Get a sample data of nyc yellow taxi to make it faster/easier to evaluate different approaches to prep for the modelling phase later in the notebook."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Import NYC yellow cab data from Azure Open Datasets\n",
							"from azureml.opendatasets import NycTlcYellow\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"\n",
							"end_date = parser.parse('2018-05-08 00:00:00')\n",
							"start_date = parser.parse('2018-05-01 00:00:00')\n",
							"\n",
							"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
							"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"#To make development easier, faster and less expensive downsample for now\n",
							"sampled_taxi_df = nyc_tlc_df.sample(True, 0.001, seed=1234)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Exploratory Data Analysis\n",
							"\n",
							"Look at the data and evaluate its suitability for use in a model, do this via some basic charts focussed on tip values and relationships."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"#The charting package needs a Pandas dataframe or numpy array do the conversion\n",
							"sampled_taxi_pd_df = sampled_taxi_df.toPandas()\n",
							"\n",
							"# Look at tips by amount count histogram\n",
							"ax1 = sampled_taxi_pd_df['tipAmount'].plot(kind='hist', bins=25, facecolor='lightblue')\n",
							"ax1.set_title('Tip amount distribution')\n",
							"ax1.set_xlabel('Tip Amount ($)')\n",
							"ax1.set_ylabel('Counts')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# How many passengers tip'd by various amounts\n",
							"ax2 = sampled_taxi_pd_df.boxplot(column=['tipAmount'], by=['passengerCount'])\n",
							"ax2.set_title('Tip amount by Passenger count')\n",
							"ax2.set_xlabel('Passenger count') \n",
							"ax2.set_ylabel('Tip Amount ($)')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# Look at the relationship between fare and tip amounts\n",
							"ax = sampled_taxi_pd_df.plot(kind='scatter', x= 'fareAmount', y = 'tipAmount', c='blue', alpha = 0.10, s=2.5*(sampled_taxi_pd_df['passengerCount']))\n",
							"ax.set_title('Tip amount by Fare amount')\n",
							"ax.set_xlabel('Fare Amount ($)')\n",
							"ax.set_ylabel('Tip Amount ($)')\n",
							"plt.axis([-2, 80, -2, 20])\n",
							"plt.suptitle('')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization\n",
							"\n",
							"It's clear from the visualizations above that there are a bunch of outliers in the data. These will need to be filtered out in addition there are extra variables that are not going to be useful in the model we build at the end.\n",
							"\n",
							"Finally there is a need to create some new (derived) variables that will work better with the model.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_df = sampled_taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'rateCodeId', 'passengerCount'\\\n",
							"                                , 'tripDistance', 'tpepPickupDateTime', 'tpepDropoffDateTime'\\\n",
							"                                , date_format('tpepPickupDateTime', 'hh').alias('pickupHour')\\\n",
							"                                , date_format('tpepPickupDateTime', 'EEEE').alias('weekdayString')\\\n",
							"                                , (unix_timestamp(col('tpepDropoffDateTime')) - unix_timestamp(col('tpepPickupDateTime'))).alias('tripTimeSecs')\\\n",
							"                                , (when(col('tipAmount') > 0, 1).otherwise(0)).alias('tipped')\n",
							"                                )\\\n",
							"                        .filter((sampled_taxi_df.passengerCount > 0) & (sampled_taxi_df.passengerCount < 8)\\\n",
							"                                & (sampled_taxi_df.tipAmount >= 0) & (sampled_taxi_df.tipAmount <= 25)\\\n",
							"                                & (sampled_taxi_df.fareAmount >= 1) & (sampled_taxi_df.fareAmount <= 250)\\\n",
							"                                & (sampled_taxi_df.tipAmount < sampled_taxi_df.fareAmount)\\\n",
							"                                & (sampled_taxi_df.tripDistance > 0) & (sampled_taxi_df.tripDistance <= 100)\\\n",
							"                                & (sampled_taxi_df.rateCodeId <= 5)\n",
							"                                & (sampled_taxi_df.paymentType.isin({\"1\", \"2\"}))\n",
							"                                )"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization Part 2\n",
							"\n",
							"Having created new variables its now possible to drop the columns they were derived from so that the dataframe that goes into the model is the smallest in terms of number of variables, that is required.\n",
							"\n",
							"Also create some more features based on new columns from the first round.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_featurised_df = taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'passengerCount'\\\n",
							"                                                , 'tripDistance', 'weekdayString', 'pickupHour','tripTimeSecs','tipped'\\\n",
							"                                                , when((taxi_df.pickupHour <= 6) | (taxi_df.pickupHour >= 20),\"Night\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 7) & (taxi_df.pickupHour <= 10), \"AMRush\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 11) & (taxi_df.pickupHour <= 15), \"Afternoon\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 16) & (taxi_df.pickupHour <= 19), \"PMRush\")\\\n",
							"                                                .otherwise(0).alias('trafficTimeBins')\n",
							"                                              )\\\n",
							"                                       .filter((taxi_df.tripTimeSecs >= 30) & (taxi_df.tripTimeSecs <= 7200))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Encoding\n",
							"\n",
							"Different ML algorithms support different types of input, for this example Logistic Regression is being used for Binary Classification. This means that any Categorical (string) variables must be converted to numbers.\n",
							"\n",
							"The process is not as simple as a \"map\" style function as the relationship between the numbers can introduce a bias in the resulting model, the approach is to index the variable and then encode using a std approach called One Hot Encoding.\n",
							"\n",
							"This approach requires the encoder to \"learn\"/fit a model over the data in the Spark instance and then transform based on what was learnt.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# The sample uses an algorithm that only works with numeric features convert them so they can be consumed\n",
							"sI1 = StringIndexer(inputCol=\"trafficTimeBins\", outputCol=\"trafficTimeBinsIndex\"); \n",
							"en1 = OneHotEncoder(dropLast=False, inputCol=\"trafficTimeBinsIndex\", outputCol=\"trafficTimeBinsVec\");\n",
							"sI2 = StringIndexer(inputCol=\"weekdayString\", outputCol=\"weekdayIndex\"); \n",
							"en2 = OneHotEncoder(dropLast=False, inputCol=\"weekdayIndex\", outputCol=\"weekdayVec\");\n",
							"\n",
							"# Create a new dataframe that has had the encodings applied\n",
							"encoded_final_df = Pipeline(stages=[sI1, en1, sI2, en2]).fit(taxi_featurised_df).transform(taxi_featurised_df)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Generation of Testing and Training Data Sets\n",
							"Simple split, 70% for training and 30% for testing the model. Playing with this ratio may result in different models.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Decide on the split between training and testing data from the dataframe \n",
							"trainingFraction = 0.7\n",
							"testingFraction = (1-trainingFraction)\n",
							"seed = 1234\n",
							"\n",
							"# Split the dataframe into test and training dataframes\n",
							"train_data_df, test_data_df = encoded_final_df.randomSplit([trainingFraction, testingFraction], seed=seed)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Train the Model\n",
							"\n",
							"Train the Logistic Regression model and then evaluate it using Area under ROC as the metric."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"## Create a new LR object for the model\n",
							"logReg = LogisticRegression(maxIter=10, regParam=0.3, labelCol = 'tipped')\n",
							"\n",
							"## The formula for the model\n",
							"classFormula = RFormula(formula=\"tipped ~ pickupHour + weekdayVec + passengerCount + tripTimeSecs + tripDistance + fareAmount + paymentType+ trafficTimeBinsVec\")\n",
							"\n",
							"## Undertake training and create an LR model\n",
							"lrModel = Pipeline(stages=[classFormula, logReg]).fit(train_data_df)\n",
							"\n",
							"## Saving the model is optional but its another for of inter session cache\n",
							"datestamp = datetime.now().strftime('%m-%d-%Y-%s');\n",
							"fileName = \"lrModel_\" + datestamp;\n",
							"logRegDirfilename = fileName;\n",
							"lrModel.save(logRegDirfilename)\n",
							"\n",
							"## Predict tip 1/0 (yes/no) on the test dataset, evaluation using AUROC\n",
							"predictions = lrModel.transform(test_data_df)\n",
							"predictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\n",
							"metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
							"print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Evaluate and Visualize\n",
							"\n",
							"Plot the actual curve to develop a better understanding of the model.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"## Plot the ROC curve, no need for pandas as this uses the modelSummary object\n",
							"modelSummary = lrModel.stages[-1].summary\n",
							"\n",
							"plt.plot([0, 1], [0, 1], 'r--')\n",
							"plt.plot(modelSummary.roc.select('FPR').collect(),\n",
							"         modelSummary.roc.select('TPR').collect())\n",
							"plt.xlabel('False Positive Rate')\n",
							"plt.ylabel('True Positive Rate')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeltaLake')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TIP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5d8b48f8-1716-49c6-b2bf-a67dcff075dd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Notebook Configuration"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(spark.conf.get(\"spark.executor.instances\"))\r\n",
							"print(spark.conf.get(\"spark.executor.cores\"))\r\n",
							"print(spark.conf.get(\"spark.executor.memory\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Note: If you request more number of vcores than  pool limit or available vcores in the pool, you will get an exception. Try reducing the numbers of vcores requested or increasing your pool size."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%%configure -f \r\n",
							"{\r\n",
							"    \"numExecutors\": 4, \r\n",
							"    \"executorCores\": 4,\r\n",
							"    \"executorMemory\": \"28g\"\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(spark.conf.get(\"spark.executor.instances\"))\r\n",
							"print(spark.conf.get(\"spark.executor.cores\"))\r\n",
							"print(spark.conf.get(\"spark.executor.memory\"))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-autoscale#get-started"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Prepare data"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import NycTlcYellow\r\n",
							"\r\n",
							"from datetime import datetime\r\n",
							"from dateutil import parser\r\n",
							"start_date = parser.parse('2010-01-01')\r\n",
							"end_date = parser.parse('2010-02-28')\r\n",
							"\r\n",
							"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\r\n",
							"nyc_tlc_df = nyc_tlc.to_spark_dataframe()\r\n",
							"\r\n",
							"# nyc_tlc_df_clean = nyc_tlc_df.drop_duplicates()\r\n",
							"nyc_tlc_df_clean = nyc_tlc_df.drop_duplicates().repartition(32)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Converting Parquet to Delta"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"nyc_tlc_df_clean.write.mode(\"overwrite\").format(\"parquet\").save(\"/data/deltademo/parquettbl\")\r\n",
							"nyc_tlc_df_clean.write.mode(\"overwrite\").format(\"parquet\").partitionBy(\"puYear\",\"puMonth\").save(\"/data/deltademo/partitionedparquettbl\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from delta.tables import *\r\n",
							"# Convert unpartitioned Parquet table at path '<path-to-table>'\r\n",
							"deltaTable = DeltaTable.convertToDelta(spark, \"parquet.`/data/deltademo/parquettbl`\")\r\n",
							"\r\n",
							"# Convert partitioned parquet table at path '<path-to-table>' and partitioned by columns with data type, separated by comma\r\n",
							"partitionedDeltaTable = DeltaTable.convertToDelta(spark, \"parquet.`/data/deltademo/partitionedparquettbl`\", \"puYear int, puMonth int\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"-- Convert unpartitioned Parquet table at path '<path-to-table>'\r\n",
							"CONVERT TO DELTA parquet.`/data/deltademo/parquettbl`;\r\n",
							"\r\n",
							"-- Convert partitioned Parquet table at path '<path-to-table>' and partitioned by columns with data type, separated by comma\r\n",
							"CONVERT TO DELTA parquet.`/data/deltademo/partitionedparquettbl` PARTITIONED BY (puYear int, puMonth int);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Working with Dataframe"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Creating managed delta tables\r\n",
							"# nyc_tlc_df_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"deltademo.deltataxitrips\")\r\n",
							"# nyc_tlc_df_clean.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"puYear\",\"puMonth\").saveAsTable(\"deltademo.deltapartitionedtaxitrips\")\r\n",
							"\r\n",
							"# Creating delta path/files\r\n",
							"nyc_tlc_df_clean.write.mode(\"overwrite\").format(\"delta\").save(\"/data/deltademo/deltataxitrips\")\r\n",
							"nyc_tlc_df_clean.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"puYear\",\"puMonth\").save(\"/data/deltademo/deltapartitionedtaxitrips\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"DROP DATABASE IF EXISTS deltademo CASCADE;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(\"CREATE DATABASE IF NOT EXISTS deltademo\")\r\n",
							"spark.sql(\"CREATE TABLE IF NOT EXISTS deltademo.deltataxitrips USING DELTA LOCATION '{0}'\".format(\"/data/deltademo/deltataxitrips\")) \r\n",
							"spark.sql(\"CREATE TABLE IF NOT EXISTS deltademo.deltapartitionedtaxitrips USING DELTA LOCATION '{0}'\".format(\"/data/deltademo/deltapartitionedtaxitrips\")) "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE DATABASE IF NOT EXISTS deltademo;\r\n",
							"\r\n",
							"CREATE TABLE IF NOT EXISTS deltademo.deltataxitrips\r\n",
							"USING DELTA\r\n",
							"LOCATION '/data/deltademo/deltataxitrips';\r\n",
							"\r\n",
							"CREATE TABLE IF NOT EXISTS deltademo.deltapartitionedtaxitrips\r\n",
							"USING DELTA\r\n",
							"LOCATION '/data/deltademo/deltapartitionedtaxitrips';"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"DESCRIBE deltademo.deltataxitrips;\r\n",
							"DESCRIBE DETAIL deltademo.deltataxitrips;\r\n",
							"\r\n",
							"DESCRIBE deltademo.deltapartitionedtaxitrips;\r\n",
							"DESCRIBE DETAIL deltademo.deltapartitionedtaxitrips;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Data Merge"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT puYear, puMonth, count(*)\r\n",
							"FROM deltademo.deltapartitionedtaxitrips \r\n",
							"GROUP BY puYear, puMonth;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azureml.opendatasets import NycTlcYellow\r\n",
							"from datetime import datetime\r\n",
							"from dateutil import parser\r\n",
							"start_date = parser.parse('2010-02-01')\r\n",
							"end_date = parser.parse('2010-03-31')\r\n",
							"nyc_tlc_incr = NycTlcYellow(start_date=start_date, end_date=end_date)\r\n",
							"nyc_tlc_incr_df = nyc_tlc_incr.to_spark_dataframe()\r\n",
							"\r\n",
							"nyc_tlc_incr_df_clean = nyc_tlc_incr_df.drop_duplicates().repartition(32)\r\n",
							"nyc_tlc_incr_df_clean.createOrReplaceTempView(\"deltaincrementaltaxitrips\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"MERGE INTO deltademo.deltapartitionedtaxitrips target\r\n",
							"USING deltaincrementaltaxitrips source\r\n",
							"ON \r\n",
							"  source.vendorID = target.vendorID AND source.tpepPickupDateTime = target.tpepPickupDateTime \r\n",
							"  AND source.tpepDropoffDateTime = target.tpepDropoffDateTime AND source.startLat = target.startLat \r\n",
							"  AND source.startLon = target.startLon AND source.endLat = target.endLat AND source.endLon = target.endLon \r\n",
							"  AND source.passengerCount = target.passengerCount AND source.tripDistance = target.tripDistance \r\n",
							"  AND source.rateCodeId = target.rateCodeId AND source.paymentType = target.paymentType \r\n",
							"  AND source.totalAmount = target.totalAmount\r\n",
							"  AND target.puYear IN (2010) AND target.puMonth IN (2,3) -- Partition Pruning \r\n",
							"WHEN MATCHED THEN\r\n",
							"  UPDATE SET \r\n",
							"    target.puLocationId = source.puLocationId\r\n",
							"    , target.doLocationId = source.doLocationId\r\n",
							"    , target.storeAndFwdFlag = source.storeAndFwdFlag\r\n",
							"    , target.fareAmount = source.fareAmount\r\n",
							"    , target.extra = source.extra\r\n",
							"    , target.mtaTax = source.mtaTax\r\n",
							"    , target.improvementSurcharge = source.improvementSurcharge\r\n",
							"    , target.tipAmount = source.tipAmount\r\n",
							"    , target.tollsAmount = source.tollsAmount\r\n",
							"WHEN NOT MATCHED\r\n",
							"  THEN INSERT (\r\n",
							"    target.vendorID, target.tpepPickupDateTime, target.tpepDropoffDateTime, target.passengerCount, target.tripDistance,\r\n",
							"    target.puLocationId, target.doLocationId, target.startLon, target.startLat, target.endLon, target.endLat, target.rateCodeId,\r\n",
							"    target.storeAndFwdFlag, target.paymentType, target.fareAmount, target.extra, target.mtaTax, target.improvementSurcharge, target.tipAmount,\r\n",
							"    target.tollsAmount, target.totalAmount, target.puYear, target.puMonth)   \r\n",
							" VALUES (\r\n",
							"   source.vendorID, source.tpepPickupDateTime, source.tpepDropoffDateTime, source.passengerCount, source.tripDistance,\r\n",
							"   source.puLocationId, source.doLocationId, source.startLon, source.startLat, source.endLon, source.endLat, source.rateCodeId, \r\n",
							"   source.storeAndFwdFlag, source.paymentType, source.fareAmount, source.extra, source.mtaTax, source.improvementSurcharge, source.tipAmount, \r\n",
							"   source.tollsAmount, source.totalAmount, source.puYear, source.puMonth)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT puYear, puMonth, count(*)\r\n",
							"FROM deltademo.deltapartitionedtaxitrips \r\n",
							"GROUP BY puYear, puMonth\r\n",
							"ORDER BY puYear, puMonth;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# from delta.tables import *\r\n",
							"\r\n",
							"# deltaTable = DeltaTable.forPath(spark, \"/data/events/\")\r\n",
							"\r\n",
							"# deltaTable.alias(\"events\").merge(\r\n",
							"#     updatesDF.alias(\"updates\"),\r\n",
							"#     \"events.eventId = updates.eventId\") \\\r\n",
							"#   .whenMatchedUpdate(set = { \"data\" : \"updates.data\" } ) \\\r\n",
							"#   .whenNotMatchedInsert(values =\r\n",
							"#     {\r\n",
							"#       \"date\": \"updates.date\",\r\n",
							"#       \"eventId\": \"updates.eventId\",\r\n",
							"#       \"data\": \"updates.data\"\r\n",
							"#     }\r\n",
							"#   ) \\\r\n",
							"#   .execute()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Time Travel - History of changes"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"DELETE FROM deltademo.deltapartitionedtaxitrips\r\n",
							"WHERE puYear = 2010 AND puMonth = 1;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT puYear, puMonth, count(*)\r\n",
							"FROM deltademo.deltapartitionedtaxitrips \r\n",
							"GROUP BY puYear, puMonth\r\n",
							"ORDER BY puYear, puMonth;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--DESCRIBE HISTORY deltademo.deltapartitionedtaxitrips; -- LIMIT 1;  -- get the last operation only\r\n",
							"DESCRIBE HISTORY delta.`/data/deltademo/deltapartitionedtaxitrips`; "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from delta.tables import *\r\n",
							"deltapartitionedtaxitrips = DeltaTable.forPath(spark, '/data/deltademo/deltapartitionedtaxitrips')\r\n",
							"display(deltapartitionedtaxitrips.history())"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"deltapartitionedtaxitrips = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/data/deltademo/deltapartitionedtaxitrips\")  \r\n",
							"display(deltapartitionedtaxitrips.groupBy(\"puYear\",\"puMonth\").count())"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"deltapartitionedtaxitrips = spark.read.format(\"delta\").option(\"timestampAsOf\", '2022-01-12 21:21:17.968').load(\"/data/deltademo/deltapartitionedtaxitrips\")  \r\n",
							"display(deltapartitionedtaxitrips.groupBy(\"puYear\",\"puMonth\").count())"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## VACCUM - Maintaining history of past data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"-- vacuum files not required by versions older than the default retention period\r\n",
							"VACUUM deltademo.deltapartitionedtaxitrips;\r\n",
							"\r\n",
							"-- vacuum files in path-based table\r\n",
							"VACUUM '/data/deltademo/deltapartitionedtaxitrips'; \r\n",
							"VACUUM delta.`/data/deltademo/deltapartitionedtaxitrips`;\r\n",
							"\r\n",
							"-- vacuum files not required by versions more than 720 hours (30 days) old\r\n",
							"VACUUM delta.`/data/deltademo/deltapartitionedtaxitrips` RETAIN 720 HOURS;\r\n",
							"\r\n",
							" -- do dry run to get the list of files to be deleted\r\n",
							"VACUUM deltademo.deltapartitionedtaxitrips DRY RUN;"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from delta.tables import *\r\n",
							"\r\n",
							"deltaTable = DeltaTable.forPath(spark, '/data/deltademo/deltapartitionedtaxitrips')  # path-based tables, or\r\n",
							"deltaTable = DeltaTable.forName(spark, 'deltademo.deltapartitionedtaxitrips')    # Hive metastore-based tables\r\n",
							"\r\n",
							"deltaTable.vacuum(720)     # vacuum files not required by versions more than 720 hours (30 days) old\r\n",
							"\r\n",
							"deltaTable.vacuum()        # vacuum files not required by versions older than the default retention period"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## File Compaction"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"path = \"/data/deltademo/deltapartitionedtaxitrips\"\r\n",
							"partition = \"puYear = '2010' and puMonth = '3'\"\r\n",
							"numFilesPerPartition = 16\r\n",
							"\r\n",
							"(spark.read\r\n",
							" .format(\"delta\")\r\n",
							" .load(path)\r\n",
							" .where(partition)\r\n",
							" .repartition(numFilesPerPartition)\r\n",
							" .write\r\n",
							" .option(\"dataChange\", \"false\")\r\n",
							" .format(\"delta\")\r\n",
							" .mode(\"overwrite\")\r\n",
							" .option(\"replaceWhere\", partition)\r\n",
							" .save(path))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Convert a Delta table to a Parquet table\r\n",
							"\r\n",
							"You can easily convert a Delta table back to a Parquet table using the following steps:\r\n",
							"\r\n",
							"If you have performed Delta Lake operations that can change the data files (for example, delete or merge), run vacuum with retention of 0 hours to delete all data files that do not belong to the latest version of the table.\r\n",
							"Delete the _delta_log directory in the table directory.\r\n",
							"\r\n",
							"**NOTE: **Delta Lake has a safety check to prevent you from running a dangerous vacuum command. If you are certain that there are no operations being performed on this table that take longer than the retention interval you plan to specify, you can turn off this safety check by setting the Apache Spark configuration property spark.databricks.delta.retentionDurationCheck.enabled to false. You must choose an interval that is longer than the longest running concurrent transaction and the longest period that any stream can lag behind the most recent update to the table.\r\n",
							"\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from delta.tables import *\r\n",
							"\r\n",
							"deltaTable = DeltaTable.forPath(spark, '/data/deltademo/deltapartitionedtaxitrips')  # path-based tables, or\r\n",
							"deltaTable = DeltaTable.forName(spark, 'deltademo.deltapartitionedtaxitrips')    # Hive metastore-based tables\r\n",
							"\r\n",
							"deltaTable.vacuum(0)        # vacuum files not required by versions older than the default retention period"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.conf.get(\"spark.databricks.delta.retentionDurationCheck.enabled\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeltaLakeOptimize')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TIP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b3d3395a-292c-4c61-8e26-73cf20cef253"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Delta Lake OPTIMIZE"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val sessionId = scala.util.Random.nextInt(1000000)\r\n",
							"val dataPath = s\"/deltaoptimizetest/data-$sessionId\";\r\n",
							"\r\n",
							"val numFiles = 1000"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Data preparation"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"spark.range(50000000).map { _ =>\r\n",
							"    (scala.util.Random.nextInt(10000000).toLong, scala.util.Random.nextInt(1000000000), scala.util.Random.nextInt(2))\r\n",
							"}.toDF(\"colA\", \"colB\", \"colC\").repartition(numFiles).write.mode(\"overwrite\").format(\"delta\").partitionBy(\"colC\").save(dataPath)\r\n",
							"\r\n",
							"// 50M rows with random integers stored in numFiles * 2 parquet files in colC=0, colC=1 partition"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"// spark.conf.get(\"spark.databricks.delta.optimize.maxFileSize\")\r\n",
							"spark.conf.get(\"spark.executor.instances\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Optimize dataset\r\n",
							"\r\n",
							"Optimize will compact small target files into larger files.\r\n",
							"Target files are\r\n",
							"- smaller than `spark.databricks.delta.optimize.maxFileSize` (default 1GB) AND\r\n",
							"- under the corresponding partitions if partition filter condition is specified.\r\n",
							"\r\n",
							"Optimize won't optimize the target files if\r\n",
							"- only one file in the partition\r\n",
							"- no candidate for compaction e.g. 2 files of 700MB & maxFileSize=1GB\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"val dt = io.delta.tables.DeltaTable.forPath(dataPath)\r\n",
							"\r\n",
							"// optimize API supports filter condition for partitioned columns. Below will optimize the files under \"colC=1\" only.\r\n",
							"dt.optimize(\"colC = 1\").show\r\n",
							"// optimize API returns statistics in Dataframe"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"// optimize all files\r\n",
							"dt.optimize().show"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Check performance without optimize"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def measure(f : => Unit) {\r\n",
							"    val start = System.nanoTime\r\n",
							"    f\r\n",
							"    val durationInMS = (System.nanoTime - start) / 1000 / 1000\r\n",
							"    println(\"duration(ms): \" + durationInMS )\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Use version 0 for non-optimized data\r\n",
							"val df = spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(dataPath)\r\n",
							"val agg = df.agg(avg(\"colA\"))\r\n",
							"measure(println(agg.collect))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Check performance after optimize"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Use the latest version for optimized data\r\n",
							"val df = spark.read.format(\"delta\").load(dataPath)\r\n",
							"val agg = df.agg(avg(\"colA\"))\r\n",
							"measure(println(agg.collect))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Spark SQL support\r\n",
							"\r\n",
							"Note: partition filter condition does not support in Spark SQL yet."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"// This won't optimize anything since the table is already optimized.\r\n",
							"spark.sql(s\"OPTIMIZE delta.`$dataPath`\").show"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### zOrder - Example"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(s\"OPTIMIZE delta.`$dataPath` ZORDER BY (ColA)\").show"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Temp Area"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"from delta.tables import *\r\n",
							"dt = DeltaTable.forPath(spark, '/deltaoptimizetest/data-246486')\r\n",
							"display(dt.optimize(condition = \"colC = 1\"))\r\n",
							"\r\n",
							"# display(dt.optimize(condition = \"colC = 1\", zOrderBy = \"colA\"))\r\n",
							"# help(dt.optimize)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(s\"CREATE TABLE sampledata USING DELTA LOCATION '$dataPath'\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"OPTIMIZE sampledata ZORDER BY (ColA)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LibraryManagement')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TIP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b5530a8f-defd-4f9a-9770-3524754efa3b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Library deployment from public repo"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pkg_resources\r\n",
							"\r\n",
							"for d in pkg_resources.working_set:\r\n",
							"    print(d)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import fuzzywuzzy\r\n",
							"print(fuzzywuzzy.__version__)\r\n",
							"print(fuzzywuzzy.__path__)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import numpy\r\n",
							"print(numpy.__version__)\r\n",
							"print(numpy.__path__)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import glob\r\n",
							"\r\n",
							"for f in glob.iglob('/usr/lib/library-manager/**/*.jar', recursive = True):\r\n",
							"    print(f)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Session level library deployment"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %%configure -f\r\n",
							"# {\r\n",
							"#     \"conf\":\r\n",
							"#     {\r\n",
							"#         \"spark.jars\": \"abfss://container@storage.dfs.core.windows.net/lib/spark-sql-kafka-0-10_2.12-3.1.1.jar, abfss://container@storage.dfs.core.windows.net/lib/spark-streaming-kafka_2.11-1.6.3.jar\"\r\n",
							"#     }\r\n",
							"# }        "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# https://mvnrepository.com/artifact/com.microsoft.azure/azure-cosmosdb-spark_2.3.0_2.11/1.3.3\r\n",
							"# <!-- https://mvnrepository.com/artifact/com.microsoft.azure/azure-cosmosdb-spark_2.3.0 -->\r\n",
							"# <dependency>\r\n",
							"#     <groupId>com.microsoft.azure</groupId>\r\n",
							"#     <artifactId>azure-cosmosdb-spark_2.3.0_2.11</artifactId>\r\n",
							"#     <version>1.3.3</version>\r\n",
							"# </dependency>"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%%configure -f \r\n",
							"{ \r\n",
							"    \"conf\": \r\n",
							"    {\r\n",
							"        \"spark.jars.packages\": \"com.microsoft.azure:azure-cosmosdb-spark_2.3.0_2.11:1.3.3\"\r\n",
							"    }\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%%configure -f \r\n",
							"{\r\n",
							"    \"spark.synapse.library.python.env\": \"name: env\\r\\nchannels:\\r\\n- anaconda\\r\\n- conda-forge\\r\\ndependencies:\\r\\n- pip:\\r\\n  - numpy\\r\\n\"\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Python - Custom Library"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pythoncalc.simplecalc.functions\r\n",
							"help(pythoncalc.simplecalc.functions)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pythoncalc.simplecalc.functions import *\r\n",
							"print(doublenum(10))\r\n",
							"print(squarenum(10))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Scala - Custom Library"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%%configure -f\r\n",
							"{\r\n",
							"    \"conf\":\r\n",
							"    {\r\n",
							"        \"spark.jars\": \"abfss://synapseintellplat@synapseintellplat.dfs.core.windows.net/library/scalapkgexample_2.12-0.1.jar\"\r\n",
							"    }\r\n",
							"}    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\r\n",
							"import scalapkgexample.scalacalc.simplecalc.Functions._\r\n",
							"\r\n",
							"doublenum(10)\r\n",
							"squarenum(10)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\r\n",
							"import org.apache.spark.SparkContext\r\n",
							"val sc = SparkContext.getOrCreate()\r\n",
							"val sourceRDD = sc.textFile(\"/data/textdata.txt\")\r\n",
							"sourceRDD.take(num = 10).foreach(println)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\r\n",
							"import scalapkgexample.scalacalc.simplecalc.ProcessData._\r\n",
							"processTextData(\"/data/textdata.txt\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\r\n",
							"import scalapkgexample.scalacalc.simplecalc.DateDimension\r\n",
							"\r\n",
							"val dateDim = new DateDimension().createDataFrame()\r\n",
							"dateDim.write.mode(\"overwrite\").saveAsTable(\"DimDate\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"show TABLES"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from default.dimdate limit 100"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import glob\r\n",
							"\r\n",
							"for f in glob.iglob('/mnt/var/hadoop/**/scalapkgexample_2.12-0.1.jar', recursive = True):\r\n",
							"    print(f)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Temp Area"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# import glob\r\n",
							"\r\n",
							"# for f in glob.iglob('/usr/lib/library-manager/**/*.jar', recursive = True):\r\n",
							"#     print(f)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# %%configure -f\r\n",
							"# {\r\n",
							"#     \"conf\":\r\n",
							"#     {\r\n",
							"#         \"spark.jars\": \"abfss://synapseintellplat@synapseintellplat.dfs.core.windows.net/library/scalapkgexample_2.12-0.1.jar\"\r\n",
							"#     }\r\n",
							"# }    "
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MultipleLoadDataFrame')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "ws1sparkpool1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "464fdf56-e9fd-4088-8999-f58711560769"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/9cd45dfd-d1ac-4c16-9363-150d2afcb462/resourceGroups/ContosoProd/providers/Microsoft.Synapse/workspaces/conktovskdzc75xepocws1/bigDataPools/ws1sparkpool1",
						"name": "ws1sparkpool1",
						"type": "Spark",
						"endpoint": "https://conktovskdzc75xepocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ws1sparkpool1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://nyctaxidata@conktovskdzc75xepoc.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://nyctaxidata@conktovskdzc75xepoc.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://nyctaxidata@conktovskdzc75xepoc.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://nyctaxidata@conktovskdzc75xepoc.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://nyctaxidata@conktovskdzc75xepoc.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://nyctaxidata@conktovskdzc75xepoc.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://nyctaxidata@conktovskdzc75xepoc.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://nyctaxidata@conktovskdzc75xepoc.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://nyctaxidata@conktovskdzc75xepoc.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://nyctaxidata@conktovskdzc75xepoc.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://nyctaxidata@conktovskdzc75xepoc.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://nyctaxidata@conktovskdzc75xepoc.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/conktovskdzc75xepocws1p1')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks",
			"apiVersion": "2019-06-01-preview",
			"properties": {},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TipSourceData')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('TipSourceData_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RawTipDataSource')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('RawTipDataSource_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('RawTipDataSource_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RawTipData')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "RawTipDataSource",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "raw"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/RawTipDataSource')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/EnrichData')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TIP"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "ws1sparkpool1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "996e1a6c-5b1a-446e-814f-04dbc244b3db"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/9cd45dfd-d1ac-4c16-9363-150d2afcb462/resourceGroups/ContosoProd/providers/Microsoft.Synapse/workspaces/conktovskdzc75xepocws1/bigDataPools/ws1sparkpool1",
						"name": "ws1sparkpool1",
						"type": "Spark",
						"endpoint": "https://conktovskdzc75xepocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ws1sparkpool1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"1. Read data from raw as csv\r\n",
							"2. remove duplicates\r\n",
							"3. remove nulls\r\n",
							"4. write back to enriched as parquet"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.types import *\r\n",
							"import datetime\r\n",
							"\r\n",
							"objDate = datetime.datetime.now()\r\n",
							"strFolder = objDate.strftime(\"%Y\") + \"_\" + objDate.strftime(\"%m\") + \"_\" + objDate.strftime(\"%d\")\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# Storage Info\r\n",
							"accountName='synapsetiphackathonadls'\r\n",
							"containerSourceName='raw'\r\n",
							"continerDestName='enriched'\r\n",
							"\r\n",
							"tableName = ''\r\n",
							"sourcePath = tableName\r\n",
							"destPath=tableName + '/' + strFolder"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": []
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.types import *\r\n",
							"import datetime\r\n",
							"\r\n",
							"print('table: ' + sourcePath)\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (containerSourceName, accountName, sourcePath)\r\n",
							"print('Source path: ' + adls_path)\r\n",
							"adls_dest_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (continerDestName, accountName, destPath)\r\n",
							"print('Dest path: ' + adls_dest_path)\r\n",
							"\r\n",
							"#read raw data as CSV\r\n",
							"df_csv = spark.read.option(\"delimiter\",\"|\").csv(adls_path, header=False)\r\n",
							"print(df_csv.count(), \"Records\")\r\n",
							"\r\n",
							"df_csv1 = df_csv.fillna('')\r\n",
							"print(df_csv1.count(), \"After replaced Nulls\")\r\n",
							"\r\n",
							"df_csv2 = df_csv1.dropDuplicates()\r\n",
							"print(df_csv2.count(), \"After removed dupes\")\r\n",
							"\r\n",
							"df_csv_coalesced = df_csv2.coalesce(1)\r\n",
							"df_csv_coalesced.write.mode(\"append\").parquet(adls_dest_path)"
						],
						"outputs": [],
						"execution_count": 59
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ws1sparkpool1')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/EnrichData')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "EnrichDataPipeline",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "EnrichData",
								"type": "NotebookReference"
							},
							"parameters": {
								"tableName": {
									"value": "customer",
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "ws1sparkpool1",
								"type": "BigDataPoolReference"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/EnrichData')]",
				"[concat(variables('workspaceId'), '/bigDataPools/ws1sparkpool1')]"
			]
		}
	]
}